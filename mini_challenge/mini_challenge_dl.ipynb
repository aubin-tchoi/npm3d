{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mini challenge - NPM3D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aubin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# @title Imports {display-mode: \"form\"}\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import jaccard_score, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.neighbors import KDTree\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "is_on_colab = False  # @param"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if is_on_colab:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(\"/content/drive\", force_remount=True)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    folder_path = \"/content/drive/MyDrive/master/mva/s2/npm3d/TP3_materials/\"  # @param\n",
    "    os.chdir(folder_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from src import (\n",
    "    FeaturesExtractor,\n",
    "    print_tensors_size_in_memory,\n",
    "    timeit,\n",
    "    checkpoint,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I/O"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file MiniLille1.ply\n",
      "767722 elements available for class Ground\n",
      "864939 elements available for class Building\n",
      "5532 elements available for class Poles\n",
      "0 elements available for class Pedestrians\n",
      "114237 elements available for class Cars\n",
      "104182 elements available for class Vegetation\n",
      "\n",
      "Reading file MiniLille2.ply\n",
      "1074040 elements available for class Ground\n",
      "1000935 elements available for class Building\n",
      "13094 elements available for class Poles\n",
      "0 elements available for class Pedestrians\n",
      "96132 elements available for class Cars\n",
      "261365 elements available for class Vegetation\n",
      "\n",
      "Reading file MiniParis1.ply\n",
      "1210261 elements available for class Ground\n",
      "1071599 elements available for class Building\n",
      "24787 elements available for class Poles\n",
      "32396 elements available for class Pedestrians\n",
      "50468 elements available for class Cars\n",
      "1455156 elements available for class Vegetation\n",
      "\n",
      "Reading file MiniDijon9.ply\n"
     ]
    }
   ],
   "source": [
    "# @title Importing the data {display-mode: \"form\"}\n",
    "\n",
    "training_path = \"./data/training\"\n",
    "test_path = \"./data/test\"\n",
    "\n",
    "f_extractor = FeaturesExtractor()\n",
    "\n",
    "(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_features,\n",
    "    test_labels,\n",
    ") = f_extractor.extract_point_clouds(training_path, \"MiniLille1.ply\")\n",
    "\n",
    "train_labels -= 1\n",
    "test_labels -= 1\n",
    "\n",
    "submission_set = f_extractor.extract_point_cloud_no_label(test_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "@timeit\n",
    "def add_neighborhoods(point_cloud: np.ndarray, n_neighbors: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Replace the [x, y, z] coordinates for each point by a 2d array\n",
    "    of the coordinates + distances to current point of the k nearest neighbors.\n",
    "    \"\"\"\n",
    "    kdtree = KDTree(point_cloud)\n",
    "    distances, neighborhoods = kdtree.query(point_cloud, k=n_neighbors)\n",
    "    resulting_cloud = np.zeros((point_cloud.shape[0], n_neighbors, 4))\n",
    "    for idx in range(point_cloud.shape[0]):\n",
    "        resulting_cloud[idx] = np.hstack(\n",
    "            (point_cloud[neighborhoods[idx]], distances[idx][:, None])\n",
    "        )\n",
    "\n",
    "    return resulting_cloud"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# @title Exporting the data {display-mode: \"form\"}\n",
    "\n",
    "\n",
    "@timeit\n",
    "def save_prediction(\n",
    "    model: nn.Module,\n",
    "    features: np.ndarray,\n",
    "    file_path: str = f\"submissions/dl-{datetime.now().strftime('%Y_%m_%d-%H_%M')}.txt\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Infers the model on the submission dataset and saves the prediction on a file.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        timer = checkpoint()\n",
    "        # running the model slice by slice to prevent our GPU from overloading\n",
    "        for idx, split in enumerate(np.array_split(features, 500)):\n",
    "            arr_slice = torch.from_numpy(split).type(torch.FloatTensor).to(device)\n",
    "            print_tensors_size_in_memory(arr_slice)\n",
    "            predictions = (\n",
    "                torch.argmax(\n",
    "                    model(arr_slice),\n",
    "                    dim=1,\n",
    "                )\n",
    "                + 1\n",
    "            )\n",
    "            np.savetxt(file_path.replace(\".txt\", f\"_{idx}.txt\"), predictions, fmt=\"%d\")\n",
    "            timer(f\"Time spent on slice {idx}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipelines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, print_loss: bool = True) -> float:\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train(True)\n",
    "\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        inputs, labels = (\n",
    "            data[\"point_cloud\"].to(device).float(),\n",
    "            data[\"category\"].to(device).long(),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # computing the prediction\n",
    "        pred = model(inputs)\n",
    "        # pred = torch.argmax(pred, dim=-2)\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 20 == 0 and print_loss:\n",
    "            loss_val, current = loss.item(), batch * len(inputs)\n",
    "            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return loss.item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn) -> Tuple[float, float]:\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, jac_score = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = (\n",
    "                data[\"point_cloud\"].to(device).float(),\n",
    "                data[\"category\"].to(device).long(),\n",
    "            )\n",
    "            pred = model(inputs)\n",
    "\n",
    "            test_loss += loss_fn(pred, labels).item()\n",
    "            jac_score += jaccard_score(\n",
    "                labels.detach().cpu().numpy().flatten(),\n",
    "                torch.argmax(pred, dim=1).detach().cpu().numpy().flatten(),\n",
    "                average=\"weighted\",\n",
    "            ) / inputs.shape[0]\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    jac_score /= num_batches\n",
    "\n",
    "    print(\n",
    "        f\"Test Error: \\n\"\n",
    "        f\" Avg loss:                {test_loss:>8f}\\n\"\n",
    "        f\" Avg Jaccard score:       {jac_score:>8f}\\n\"\n",
    "    )\n",
    "\n",
    "    return test_loss, jac_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def plot_performance(loss_train, loss_test, jac_score):\n",
    "    \"\"\"\n",
    "    Plots three graphs describing the evolution of three metrics duering the training process.\n",
    "    \"\"\"\n",
    "    # subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # loss on training dataset\n",
    "    ax1.plot(loss_train, color=\"red\")\n",
    "    ax1.set_title(\"Model loss on training set\")\n",
    "    ax1.set_ylabel(\"Loss on training set\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "    # Jaccard score on validation dataset\n",
    "    ax2.plot(jac_score, color=\"red\")\n",
    "    ax2.set_title(\"Jaccard score on validation set\")\n",
    "    ax2.set_ylabel(\"Jaccard score on validation set\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "\n",
    "    # loss on validation dataset\n",
    "    ax3.plot(loss_test, color=\"red\")\n",
    "    ax3.set_title(\"Model loss on validation set\")\n",
    "    ax3.set_ylabel(\"Loss on validation set\")\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def check_few_batches(model, train_dataloader, n_batches: int = 5):\n",
    "    \"\"\"\n",
    "    Displays the accuracy and the confusion matrix on a few batches.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(train_dataloader):\n",
    "            pred = torch.argmax(model(x.to(device)).cpu(), dim=1)\n",
    "            print(f\"Accuracy on batch {i}: {accuracy_score(y, pred)}\")\n",
    "            ConfusionMatrixDisplay.from_predictions(y, pred)\n",
    "            if i == n_batches:\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=1)\n",
    "\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, eval_mode=False):\n",
    "        self.data = train_features if not eval_mode else test_features\n",
    "        self.labels = train_labels if not eval_mode else test_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return 16 * len(self.data) // 4096\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = rng.choice(len(self.data), 4096, replace=False)\n",
    "        return {\"point_cloud\": self.data[indices], \"category\": self.labels[indices]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "train_dataset = PointCloudDataset()\n",
    "# no need to shuffle as the point are drawn randomly\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=42,\n",
    ")\n",
    "test_dataset = PointCloudDataset(eval_mode=True)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=42,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, classes=6):\n",
    "        super(PointNet, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 64, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(64, 64, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv5 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, classes)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(1024)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn6(self.fc1(x)))\n",
    "        x = F.relu(self.bn7(self.fc2(x)))\n",
    "        x = self.dropout(self.fc3(x))\n",
    "\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class PointNetSeg(nn.Module):\n",
    "    def __init__(self, classes=6):\n",
    "        super(PointNetSeg, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 64, 1)\n",
    "\n",
    "        self.conv3 = torch.nn.Conv1d(64, 64, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv5 = torch.nn.Conv1d(128, 1024, 1)\n",
    "\n",
    "        self.conv6 = torch.nn.Conv1d(1088, 512, 1)\n",
    "        self.conv7 = torch.nn.Conv1d(512, 256, 1)\n",
    "        self.conv8 = torch.nn.Conv1d(256, 128, 1)\n",
    "\n",
    "        self.conv9 = torch.nn.Conv1d(128, 128, 1)\n",
    "        self.conv10 = torch.nn.Conv1d(128, classes, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(1024)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        self.bn8 = nn.BatchNorm1d(128)\n",
    "        self.bn9 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        local_feat = x.transpose(1, 2)\n",
    "        local_feat = F.relu(self.bn1(self.conv1(local_feat)))\n",
    "        local_feat = F.relu(self.bn2(self.conv2(local_feat)))\n",
    "\n",
    "        global_feat = F.relu(self.bn3(self.conv3(local_feat)))\n",
    "        global_feat = F.relu(self.bn4(self.conv4(global_feat)))\n",
    "        global_feat = F.relu(self.bn5(self.conv5(global_feat)))\n",
    "\n",
    "        global_feat = torch.max(global_feat, 2, keepdim=True)[0]\n",
    "        global_feat = global_feat.view(-1, 1024)\n",
    "\n",
    "        global_feat = global_feat.repeat((1, 4096)).view(-1, 1024, 4096)\n",
    "\n",
    "        point_features = torch.cat((local_feat, global_feat), -2)\n",
    "\n",
    "        point_features = F.relu(self.bn6(self.conv6(point_features)))\n",
    "        point_features = F.relu(self.bn7(self.conv7(point_features)))\n",
    "        point_features = F.relu(self.bn8(self.conv8(point_features)))\n",
    "        point_features = F.relu(self.bn9(self.conv9(point_features)))\n",
    "        point_features = self.conv10(point_features)\n",
    "\n",
    "        return point_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointNetSeg(\n",
      "  (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv4): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "  (conv5): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "  (conv6): Conv1d(1088, 512, kernel_size=(1,), stride=(1,))\n",
      "  (conv7): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "  (conv8): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "  (conv9): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "  (conv10): Conv1d(128, 6, kernel_size=(1,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = PointNetSeg().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "max_epochs = 800\n",
    "epoch = 0\n",
    "\n",
    "loss_list_train, loss_list_test, jac_score_list_test = [], [], []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.011138  [    0/ 1837]\n",
      "loss: 0.014124  [  840/ 1837]\n",
      "loss: 0.020706  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                50.576690\n",
      " Avg Jaccard score:       0.002416\n",
      "\n",
      "Epoch 10 lasted for: 16746.5434\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.026185  [    0/ 1837]\n",
      "loss: 0.013916  [  840/ 1837]\n",
      "loss: 0.013875  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                32.667823\n",
      " Avg Jaccard score:       0.004713\n",
      "\n",
      "Epoch 11 lasted for: 20.8942\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.012122  [    0/ 1837]\n",
      "loss: 0.011264  [  840/ 1837]\n",
      "loss: 0.011589  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                38.667194\n",
      " Avg Jaccard score:       0.006523\n",
      "\n",
      "Epoch 12 lasted for: 20.6934\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.009669  [    0/ 1837]\n",
      "loss: 0.037787  [  840/ 1837]\n",
      "loss: 0.019574  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                26.479667\n",
      " Avg Jaccard score:       0.007173\n",
      "\n",
      "Epoch 13 lasted for: 20.6018\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.023310  [    0/ 1837]\n",
      "loss: 0.011663  [  840/ 1837]\n",
      "loss: 0.009916  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                39.038365\n",
      " Avg Jaccard score:       0.005303\n",
      "\n",
      "Epoch 14 lasted for: 20.6164\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.010219  [    0/ 1837]\n",
      "loss: 0.011769  [  840/ 1837]\n",
      "loss: 0.010295  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                30.923680\n",
      " Avg Jaccard score:       0.008299\n",
      "\n",
      "Epoch 15 lasted for: 20.7027\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.008693  [    0/ 1837]\n",
      "loss: 0.009215  [  840/ 1837]\n",
      "loss: 0.007966  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                32.892388\n",
      " Avg Jaccard score:       0.005777\n",
      "\n",
      "Epoch 16 lasted for: 20.7193\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.007929  [    0/ 1837]\n",
      "loss: 0.008518  [  840/ 1837]\n",
      "loss: 0.009655  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                30.181410\n",
      " Avg Jaccard score:       0.006388\n",
      "\n",
      "Epoch 17 lasted for: 20.7455\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.008200  [    0/ 1837]\n",
      "loss: 0.010254  [  840/ 1837]\n",
      "loss: 0.010328  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                39.382710\n",
      " Avg Jaccard score:       0.006017\n",
      "\n",
      "Epoch 18 lasted for: 20.7436\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.008022  [    0/ 1837]\n",
      "loss: 0.008151  [  840/ 1837]\n",
      "loss: 0.007699  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                31.491657\n",
      " Avg Jaccard score:       0.006975\n",
      "\n",
      "Epoch 19 lasted for: 20.8909\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.007968  [    0/ 1837]\n",
      "loss: 0.007220  [  840/ 1837]\n",
      "loss: 0.008843  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                32.792463\n",
      " Avg Jaccard score:       0.006727\n",
      "\n",
      "Epoch 20 lasted for: 20.7295\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.010764  [    0/ 1837]\n",
      "loss: 0.010194  [  840/ 1837]\n",
      "loss: 0.007896  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                30.329709\n",
      " Avg Jaccard score:       0.007181\n",
      "\n",
      "Epoch 21 lasted for: 20.7574\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.008312  [    0/ 1837]\n",
      "loss: 0.009215  [  840/ 1837]\n",
      "loss: 0.008543  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                37.798150\n",
      " Avg Jaccard score:       0.006219\n",
      "\n",
      "Epoch 22 lasted for: 20.8029\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.008839  [    0/ 1837]\n",
      "loss: 0.011190  [  840/ 1837]\n",
      "loss: 0.008530  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                32.082480\n",
      " Avg Jaccard score:       0.006663\n",
      "\n",
      "Epoch 23 lasted for: 20.8301\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.008133  [    0/ 1837]\n",
      "loss: 0.008143  [  840/ 1837]\n",
      "loss: 0.007934  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                35.236956\n",
      " Avg Jaccard score:       0.007063\n",
      "\n",
      "Epoch 24 lasted for: 20.7958\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.011381  [    0/ 1837]\n",
      "loss: 0.009395  [  840/ 1837]\n",
      "loss: 0.008722  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                30.032456\n",
      " Avg Jaccard score:       0.006549\n",
      "\n",
      "Epoch 25 lasted for: 20.7817\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.010747  [    0/ 1837]\n",
      "loss: 0.008925  [  840/ 1837]\n",
      "loss: 0.007422  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                46.692435\n",
      " Avg Jaccard score:       0.006083\n",
      "\n",
      "Epoch 26 lasted for: 20.8015\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.008104  [    0/ 1837]\n",
      "loss: 0.009215  [  840/ 1837]\n",
      "loss: 0.009289  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                41.361937\n",
      " Avg Jaccard score:       0.007454\n",
      "\n",
      "Epoch 27 lasted for: 20.8031\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.008589  [    0/ 1837]\n",
      "loss: 0.008432  [  840/ 1837]\n",
      "loss: 0.008120  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                34.402771\n",
      " Avg Jaccard score:       0.006930\n",
      "\n",
      "Epoch 28 lasted for: 20.8073\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.010202  [    0/ 1837]\n",
      "loss: 0.009431  [  840/ 1837]\n",
      "loss: 0.010059  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                34.163341\n",
      " Avg Jaccard score:       0.005522\n",
      "\n",
      "Epoch 29 lasted for: 20.7905\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.009501  [    0/ 1837]\n",
      "loss: 0.008683  [  840/ 1837]\n",
      "loss: 0.007897  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                37.157871\n",
      " Avg Jaccard score:       0.006647\n",
      "\n",
      "Epoch 30 lasted for: 20.8439\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.008281  [    0/ 1837]\n",
      "loss: 0.015347  [  840/ 1837]\n",
      "loss: 0.009624  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                40.738432\n",
      " Avg Jaccard score:       0.005032\n",
      "\n",
      "Epoch 31 lasted for: 20.8773\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.009461  [    0/ 1837]\n",
      "loss: 0.013128  [  840/ 1837]\n",
      "loss: 0.009110  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                31.332737\n",
      " Avg Jaccard score:       0.006407\n",
      "\n",
      "Epoch 32 lasted for: 20.7642\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.008807  [    0/ 1837]\n",
      "loss: 0.008380  [  840/ 1837]\n",
      "loss: 0.007700  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                33.277075\n",
      " Avg Jaccard score:       0.007778\n",
      "\n",
      "Epoch 33 lasted for: 20.8334\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.007811  [    0/ 1837]\n",
      "loss: 0.009654  [  840/ 1837]\n",
      "loss: 0.009397  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                35.068760\n",
      " Avg Jaccard score:       0.005386\n",
      "\n",
      "Epoch 34 lasted for: 20.8007\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.007994  [    0/ 1837]\n",
      "loss: 0.006678  [  840/ 1837]\n",
      "loss: 0.006583  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                31.395357\n",
      " Avg Jaccard score:       0.007494\n",
      "\n",
      "Epoch 35 lasted for: 20.9153\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.010811  [    0/ 1837]\n",
      "loss: 0.007934  [  840/ 1837]\n",
      "loss: 0.010480  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                25.581434\n",
      " Avg Jaccard score:       0.007823\n",
      "\n",
      "Epoch 36 lasted for: 20.9295\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.009864  [    0/ 1837]\n",
      "loss: 0.007299  [  840/ 1837]\n",
      "loss: 0.006959  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                32.910243\n",
      " Avg Jaccard score:       0.006847\n",
      "\n",
      "Epoch 37 lasted for: 20.8879\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.007722  [    0/ 1837]\n",
      "loss: 0.007584  [  840/ 1837]\n",
      "loss: 0.007929  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                33.135758\n",
      " Avg Jaccard score:       0.007582\n",
      "\n",
      "Epoch 38 lasted for: 20.8949\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.009405  [    0/ 1837]\n",
      "loss: 0.008294  [  840/ 1837]\n",
      "loss: 0.007601  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                32.493312\n",
      " Avg Jaccard score:       0.007782\n",
      "\n",
      "Epoch 39 lasted for: 20.7951\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.007548  [    0/ 1837]\n",
      "loss: 0.007690  [  840/ 1837]\n",
      "loss: 0.008194  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                34.573944\n",
      " Avg Jaccard score:       0.006885\n",
      "\n",
      "Epoch 40 lasted for: 20.7602\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.007754  [    0/ 1837]\n",
      "loss: 0.007468  [  840/ 1837]\n",
      "loss: 0.009396  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                36.826398\n",
      " Avg Jaccard score:       0.007964\n",
      "\n",
      "Epoch 41 lasted for: 20.8362\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.007850  [    0/ 1837]\n",
      "loss: 0.006974  [  840/ 1837]\n",
      "loss: 0.007151  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                33.770104\n",
      " Avg Jaccard score:       0.007297\n",
      "\n",
      "Epoch 42 lasted for: 20.9458\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.007298  [    0/ 1837]\n",
      "loss: 0.007168  [  840/ 1837]\n",
      "loss: 0.008644  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                35.420405\n",
      " Avg Jaccard score:       0.007567\n",
      "\n",
      "Epoch 43 lasted for: 20.8752\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.008172  [    0/ 1837]\n",
      "loss: 0.007264  [  840/ 1837]\n",
      "loss: 0.009093  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                40.447027\n",
      " Avg Jaccard score:       0.007659\n",
      "\n",
      "Epoch 44 lasted for: 20.8826\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.011108  [    0/ 1837]\n",
      "loss: 0.007001  [  840/ 1837]\n",
      "loss: 0.007034  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                36.725260\n",
      " Avg Jaccard score:       0.007668\n",
      "\n",
      "Epoch 45 lasted for: 20.8771\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.007379  [    0/ 1837]\n",
      "loss: 0.007049  [  840/ 1837]\n",
      "loss: 0.008766  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                37.515869\n",
      " Avg Jaccard score:       0.007605\n",
      "\n",
      "Epoch 46 lasted for: 20.8570\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.007395  [    0/ 1837]\n",
      "loss: 0.009035  [  840/ 1837]\n",
      "loss: 0.007920  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                38.877392\n",
      " Avg Jaccard score:       0.006971\n",
      "\n",
      "Epoch 47 lasted for: 20.8319\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.009612  [    0/ 1837]\n",
      "loss: 0.006965  [  840/ 1837]\n",
      "loss: 0.009039  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                35.084439\n",
      " Avg Jaccard score:       0.006781\n",
      "\n",
      "Epoch 48 lasted for: 20.8357\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.012045  [    0/ 1837]\n",
      "loss: 0.011988  [  840/ 1837]\n",
      "loss: 0.011178  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                37.528179\n",
      " Avg Jaccard score:       0.008277\n",
      "\n",
      "Epoch 49 lasted for: 20.8813\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.007841  [    0/ 1837]\n",
      "loss: 0.006685  [  840/ 1837]\n",
      "loss: 0.007737  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                37.899712\n",
      " Avg Jaccard score:       0.008871\n",
      "\n",
      "Epoch 50 lasted for: 20.8802\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.007315  [    0/ 1837]\n",
      "loss: 0.007436  [  840/ 1837]\n",
      "loss: 0.006891  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                33.596424\n",
      " Avg Jaccard score:       0.007877\n",
      "\n",
      "Epoch 51 lasted for: 20.9156\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.008542  [    0/ 1837]\n",
      "loss: 0.006569  [  840/ 1837]\n",
      "loss: 0.007076  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                35.476742\n",
      " Avg Jaccard score:       0.007275\n",
      "\n",
      "Epoch 52 lasted for: 20.8859\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.007982  [    0/ 1837]\n",
      "loss: 0.007434  [  840/ 1837]\n",
      "loss: 0.006247  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                35.312160\n",
      " Avg Jaccard score:       0.006663\n",
      "\n",
      "Epoch 53 lasted for: 20.8506\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.007316  [    0/ 1837]\n",
      "loss: 0.006997  [  840/ 1837]\n",
      "loss: 0.007002  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                43.235449\n",
      " Avg Jaccard score:       0.006536\n",
      "\n",
      "Epoch 54 lasted for: 20.8254\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.007754  [    0/ 1837]\n",
      "loss: 0.006288  [  840/ 1837]\n",
      "loss: 0.007146  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                35.791826\n",
      " Avg Jaccard score:       0.007529\n",
      "\n",
      "Epoch 55 lasted for: 20.8332\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.006840  [    0/ 1837]\n",
      "loss: 0.006431  [  840/ 1837]\n",
      "loss: 0.006033  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                34.281472\n",
      " Avg Jaccard score:       0.008217\n",
      "\n",
      "Epoch 56 lasted for: 20.9098\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.005958  [    0/ 1837]\n",
      "loss: 0.006723  [  840/ 1837]\n",
      "loss: 0.006376  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                39.035998\n",
      " Avg Jaccard score:       0.008070\n",
      "\n",
      "Epoch 57 lasted for: 20.8657\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.006727  [    0/ 1837]\n",
      "loss: 0.006132  [  840/ 1837]\n",
      "loss: 0.006201  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                36.429487\n",
      " Avg Jaccard score:       0.007313\n",
      "\n",
      "Epoch 58 lasted for: 20.8565\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.007552  [    0/ 1837]\n",
      "loss: 0.006360  [  840/ 1837]\n",
      "loss: 0.007939  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                37.930294\n",
      " Avg Jaccard score:       0.007125\n",
      "\n",
      "Epoch 59 lasted for: 20.8511\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.009732  [    0/ 1837]\n",
      "loss: 0.007348  [  840/ 1837]\n",
      "loss: 0.006973  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                40.350664\n",
      " Avg Jaccard score:       0.007417\n",
      "\n",
      "Epoch 60 lasted for: 20.8398\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.007442  [    0/ 1837]\n",
      "loss: 0.006569  [  840/ 1837]\n",
      "loss: 0.007014  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                41.910758\n",
      " Avg Jaccard score:       0.007202\n",
      "\n",
      "Epoch 61 lasted for: 20.8826\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.005713  [    0/ 1837]\n",
      "loss: 0.010895  [  840/ 1837]\n",
      "loss: 0.007297  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                40.738698\n",
      " Avg Jaccard score:       0.008105\n",
      "\n",
      "Epoch 62 lasted for: 20.8927\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.008354  [    0/ 1837]\n",
      "loss: 0.005867  [  840/ 1837]\n",
      "loss: 0.008203  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                39.074243\n",
      " Avg Jaccard score:       0.008239\n",
      "\n",
      "Epoch 63 lasted for: 20.9190\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.010780  [    0/ 1837]\n",
      "loss: 0.008020  [  840/ 1837]\n",
      "loss: 0.007709  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                39.983014\n",
      " Avg Jaccard score:       0.007651\n",
      "\n",
      "Epoch 64 lasted for: 20.9131\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.006902  [    0/ 1837]\n",
      "loss: 0.006307  [  840/ 1837]\n",
      "loss: 0.006579  [ 1680/ 1837]\n",
      "Test Error: \n",
      " Avg loss:                40.385250\n",
      " Avg Jaccard score:       0.007153\n",
      "\n",
      "Epoch 65 lasted for: 20.8836\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.006474  [    0/ 1837]\n",
      "loss: 0.008071  [  840/ 1837]\n"
     ]
    }
   ],
   "source": [
    "# you can stop this loop with a KeyboardInterrupt and rerun it to pick up where you left off\n",
    "timer = checkpoint()\n",
    "while epoch < max_epochs:\n",
    "    try:\n",
    "        print(f\"\\nEpoch {epoch + 1}\\n-------------------------------\")\n",
    "\n",
    "        loss_train = train(train_dataloader, model, loss_fn, optimizer)\n",
    "        scheduler.step()\n",
    "        test_loss, jac_score = test(test_dataloader, model, loss_fn)\n",
    "\n",
    "        timer(f\"Epoch {epoch + 1} lasted for\")\n",
    "\n",
    "        loss_list_train.append(loss_train)\n",
    "        loss_list_test.append(test_loss)\n",
    "        jac_score_list_test.append(jac_score)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExecution stopped.\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_performance(loss_list_train, loss_list_test, jac_score_list_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "check_few_batches(model, train_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Saving the trained model {display-mode: \"form\"}\n",
    "\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    f\"submissions/model-{datetime.now().strftime('%Y_%m_%d-%H_%M')}.pth\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_prediction(model, submission_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
